
==================== Loading and Splitting Data ====================
Task A (Classes 0-4): 25000 training samples
Task B (Classes 5-9): 25000 training samples

==================== EXPERIMENT 1: Full Fine-Tuning (The Baseline) ====================
Training on Task A...
{'loss': 1.2712, 'grad_norm': 37.67932891845703, 'learning_rate': 4.937340153452686e-05, 'epoch': 0.06}
{'loss': 0.6627, 'grad_norm': 18.372987747192383, 'learning_rate': 4.8734015345268545e-05, 'epoch': 0.13}
{'loss': 0.5366, 'grad_norm': 33.23933029174805, 'learning_rate': 4.809462915601023e-05, 'epoch': 0.19}
{'loss': 0.4866, 'grad_norm': 23.467044830322266, 'learning_rate': 4.745524296675192e-05, 'epoch': 0.26}
{'loss': 0.4234, 'grad_norm': 7.85373592376709, 'learning_rate': 4.681585677749361e-05, 'epoch': 0.32}
{'loss': 0.3885, 'grad_norm': 19.55356216430664, 'learning_rate': 4.61764705882353e-05, 'epoch': 0.38}
{'loss': 0.3851, 'grad_norm': 27.246667861938477, 'learning_rate': 4.5537084398976985e-05, 'epoch': 0.45}
{'loss': 0.4225, 'grad_norm': 39.38937759399414, 'learning_rate': 4.489769820971867e-05, 'epoch': 0.51}
{'loss': 0.3701, 'grad_norm': 21.920429229736328, 'learning_rate': 4.4258312020460354e-05, 'epoch': 0.58}
{'loss': 0.3636, 'grad_norm': 30.483970642089844, 'learning_rate': 4.361892583120204e-05, 'epoch': 0.64}
{'loss': 0.3417, 'grad_norm': 5.6532416343688965, 'learning_rate': 4.297953964194373e-05, 'epoch': 0.7}
{'loss': 0.318, 'grad_norm': 15.88342571258545, 'learning_rate': 4.234015345268542e-05, 'epoch': 0.77}
{'loss': 0.3165, 'grad_norm': 14.885770797729492, 'learning_rate': 4.1700767263427106e-05, 'epoch': 0.83}
{'loss': 0.2864, 'grad_norm': 11.65827465057373, 'learning_rate': 4.1061381074168794e-05, 'epoch': 0.9}
{'loss': 0.324, 'grad_norm': 16.468307495117188, 'learning_rate': 4.042199488491048e-05, 'epoch': 0.96}
{'loss': 0.2771, 'grad_norm': 24.815284729003906, 'learning_rate': 3.978260869565217e-05, 'epoch': 1.02}
{'loss': 0.2253, 'grad_norm': 25.921838760375977, 'learning_rate': 3.914322250639386e-05, 'epoch': 1.09}
{'loss': 0.209, 'grad_norm': 16.71416664123535, 'learning_rate': 3.8503836317135546e-05, 'epoch': 1.15}
{'loss': 0.2131, 'grad_norm': 33.941402435302734, 'learning_rate': 3.7864450127877235e-05, 'epoch': 1.21}
{'loss': 0.2306, 'grad_norm': 9.096193313598633, 'learning_rate': 3.722506393861892e-05, 'epoch': 1.28}
{'loss': 0.2449, 'grad_norm': 8.57613754272461, 'learning_rate': 3.658567774936061e-05, 'epoch': 1.34}
{'loss': 0.2483, 'grad_norm': 20.13690185546875, 'learning_rate': 3.5946291560102305e-05, 'epoch': 1.41}
{'loss': 0.2629, 'grad_norm': 35.038326263427734, 'learning_rate': 3.5306905370843993e-05, 'epoch': 1.47}
{'loss': 0.2253, 'grad_norm': 20.269472122192383, 'learning_rate': 3.466751918158568e-05, 'epoch': 1.53}
{'loss': 0.2098, 'grad_norm': 29.303571701049805, 'learning_rate': 3.402813299232737e-05, 'epoch': 1.6}
{'loss': 0.2006, 'grad_norm': 16.985559463500977, 'learning_rate': 3.338874680306906e-05, 'epoch': 1.66}
{'loss': 0.2046, 'grad_norm': 15.525482177734375, 'learning_rate': 3.2749360613810746e-05, 'epoch': 1.73}
{'loss': 0.1832, 'grad_norm': 35.45729446411133, 'learning_rate': 3.2109974424552434e-05, 'epoch': 1.79}
{'loss': 0.2362, 'grad_norm': 33.975547790527344, 'learning_rate': 3.147058823529412e-05, 'epoch': 1.85}
{'loss': 0.1675, 'grad_norm': 12.534250259399414, 'learning_rate': 3.083120204603581e-05, 'epoch': 1.92}
{'loss': 0.1989, 'grad_norm': 12.792533874511719, 'learning_rate': 3.0191815856777494e-05, 'epoch': 1.98}
{'loss': 0.1409, 'grad_norm': 0.31540173292160034, 'learning_rate': 2.9552429667519182e-05, 'epoch': 2.05}
{'loss': 0.1496, 'grad_norm': 3.483548164367676, 'learning_rate': 2.891304347826087e-05, 'epoch': 2.11}
{'loss': 0.1066, 'grad_norm': 12.917648315429688, 'learning_rate': 2.827365728900256e-05, 'epoch': 2.17}
{'loss': 0.1073, 'grad_norm': 29.498554229736328, 'learning_rate': 2.7634271099744246e-05, 'epoch': 2.24}
{'loss': 0.1156, 'grad_norm': 19.803043365478516, 'learning_rate': 2.6994884910485935e-05, 'epoch': 2.3}
{'loss': 0.1198, 'grad_norm': 38.956565856933594, 'learning_rate': 2.6355498721227623e-05, 'epoch': 2.37}
{'loss': 0.1229, 'grad_norm': 21.888338088989258, 'learning_rate': 2.571611253196931e-05, 'epoch': 2.43}
{'loss': 0.1427, 'grad_norm': 12.14199161529541, 'learning_rate': 2.5076726342711e-05, 'epoch': 2.49}
{'loss': 0.0995, 'grad_norm': 1.9333925247192383, 'learning_rate': 2.4437340153452687e-05, 'epoch': 2.56}
{'loss': 0.1674, 'grad_norm': 10.233416557312012, 'learning_rate': 2.3797953964194375e-05, 'epoch': 2.62}
{'loss': 0.1222, 'grad_norm': 2.509415626525879, 'learning_rate': 2.3158567774936063e-05, 'epoch': 2.69}
{'loss': 0.1162, 'grad_norm': 29.02067756652832, 'learning_rate': 2.251918158567775e-05, 'epoch': 2.75}
{'loss': 0.1022, 'grad_norm': 19.413433074951172, 'learning_rate': 2.187979539641944e-05, 'epoch': 2.81}
{'loss': 0.1016, 'grad_norm': 3.410443067550659, 'learning_rate': 2.1240409207161127e-05, 'epoch': 2.88}
{'loss': 0.1241, 'grad_norm': 7.296327114105225, 'learning_rate': 2.0601023017902815e-05, 'epoch': 2.94}
{'loss': 0.1247, 'grad_norm': 8.780552864074707, 'learning_rate': 1.9961636828644503e-05, 'epoch': 3.01}
{'loss': 0.0468, 'grad_norm': 17.529312133789062, 'learning_rate': 1.932225063938619e-05, 'epoch': 3.07}
{'loss': 0.0592, 'grad_norm': 7.418809413909912, 'learning_rate': 1.868286445012788e-05, 'epoch': 3.13}
{'loss': 0.0515, 'grad_norm': 4.029571056365967, 'learning_rate': 1.8043478260869567e-05, 'epoch': 3.2}
{'loss': 0.0575, 'grad_norm': 46.15746307373047, 'learning_rate': 1.740409207161125e-05, 'epoch': 3.26}
{'loss': 0.0431, 'grad_norm': 5.219450950622559, 'learning_rate': 1.676470588235294e-05, 'epoch': 3.32}
{'loss': 0.0554, 'grad_norm': 1.685835838317871, 'learning_rate': 1.6125319693094628e-05, 'epoch': 3.39}
{'loss': 0.0561, 'grad_norm': 19.777725219726562, 'learning_rate': 1.5485933503836316e-05, 'epoch': 3.45}
{'loss': 0.044, 'grad_norm': 2.7928147315979004, 'learning_rate': 1.4846547314578006e-05, 'epoch': 3.52}
{'loss': 0.0375, 'grad_norm': 2.619941473007202, 'learning_rate': 1.4207161125319694e-05, 'epoch': 3.58}
{'loss': 0.0406, 'grad_norm': 34.301536560058594, 'learning_rate': 1.3567774936061382e-05, 'epoch': 3.64}
{'loss': 0.0387, 'grad_norm': 2.7027063369750977, 'learning_rate': 1.292838874680307e-05, 'epoch': 3.71}
{'loss': 0.0674, 'grad_norm': 15.673962593078613, 'learning_rate': 1.2289002557544758e-05, 'epoch': 3.77}
{'loss': 0.0664, 'grad_norm': 32.7398567199707, 'learning_rate': 1.1649616368286446e-05, 'epoch': 3.84}
{'loss': 0.0434, 'grad_norm': 0.1960192322731018, 'learning_rate': 1.1010230179028134e-05, 'epoch': 3.9}
{'loss': 0.0433, 'grad_norm': 31.758766174316406, 'learning_rate': 1.0370843989769822e-05, 'epoch': 3.96}
{'loss': 0.0211, 'grad_norm': 0.4649334251880646, 'learning_rate': 9.73145780051151e-06, 'epoch': 4.03}
{'loss': 0.0114, 'grad_norm': 0.7514796853065491, 'learning_rate': 9.092071611253198e-06, 'epoch': 4.09}
{'loss': 0.007, 'grad_norm': 0.0248657688498497, 'learning_rate': 8.452685421994886e-06, 'epoch': 4.16}
{'loss': 0.0144, 'grad_norm': 0.22374123334884644, 'learning_rate': 7.813299232736574e-06, 'epoch': 4.22}
{'loss': 0.0101, 'grad_norm': 0.01656627282500267, 'learning_rate': 7.173913043478261e-06, 'epoch': 4.28}
{'loss': 0.005, 'grad_norm': 10.932096481323242, 'learning_rate': 6.534526854219949e-06, 'epoch': 4.35}
{'loss': 0.015, 'grad_norm': 0.00021742678654845804, 'learning_rate': 5.895140664961637e-06, 'epoch': 4.41}
{'loss': 0.012, 'grad_norm': 0.00030753432656638324, 'learning_rate': 5.255754475703325e-06, 'epoch': 4.48}
{'loss': 0.0044, 'grad_norm': 0.000450612889835611, 'learning_rate': 4.616368286445013e-06, 'epoch': 4.54}
{'loss': 0.0206, 'grad_norm': 0.004884575959295034, 'learning_rate': 3.976982097186701e-06, 'epoch': 4.6}
{'loss': 0.0056, 'grad_norm': 0.13276103138923645, 'learning_rate': 3.337595907928389e-06, 'epoch': 4.67}
{'loss': 0.01, 'grad_norm': 5.179227446205914e-05, 'learning_rate': 2.698209718670077e-06, 'epoch': 4.73}
{'loss': 0.0035, 'grad_norm': 0.002271219389513135, 'learning_rate': 2.058823529411765e-06, 'epoch': 4.8}
{'loss': 0.0003, 'grad_norm': 4.189915657043457, 'learning_rate': 1.4194373401534528e-06, 'epoch': 4.86}
{'loss': 0.0039, 'grad_norm': 0.2583775818347931, 'learning_rate': 7.800511508951407e-07, 'epoch': 4.92}
{'loss': 0.0079, 'grad_norm': 0.0034215180203318596, 'learning_rate': 1.4066496163682866e-07, 'epoch': 4.99}
{'train_runtime': 652.8741, 'train_samples_per_second': 191.461, 'train_steps_per_second': 5.989, 'train_loss': 0.17008446020400508, 'epoch': 5.0}
Evaluating on Test A (After Training A)...
Accuracy/Loss on Test A (After Training A): {'eval_loss': 0.18136301636695862, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 16.5024, 'eval_samples_per_second': 302.986, 'eval_steps_per_second': 37.873}
Training on Task B (Continual Step)...
{'loss': 1.8323, 'grad_norm': 3.7752935886383057, 'learning_rate': 4.937340153452686e-05, 'epoch': 0.06}
{'loss': 0.4406, 'grad_norm': 23.06389045715332, 'learning_rate': 4.8734015345268545e-05, 'epoch': 0.13}
{'loss': 0.3774, 'grad_norm': 12.769078254699707, 'learning_rate': 4.809462915601023e-05, 'epoch': 0.19}
{'loss': 0.3178, 'grad_norm': 7.25039529800415, 'learning_rate': 4.745524296675192e-05, 'epoch': 0.26}
{'loss': 0.267, 'grad_norm': 9.730036735534668, 'learning_rate': 4.681585677749361e-05, 'epoch': 0.32}
{'loss': 0.2401, 'grad_norm': 19.396743774414062, 'learning_rate': 4.61764705882353e-05, 'epoch': 0.38}
{'loss': 0.2681, 'grad_norm': 14.36218547821045, 'learning_rate': 4.5537084398976985e-05, 'epoch': 0.45}
{'loss': 0.2611, 'grad_norm': 15.38299560546875, 'learning_rate': 4.489769820971867e-05, 'epoch': 0.51}
{'loss': 0.2362, 'grad_norm': 13.015975952148438, 'learning_rate': 4.4258312020460354e-05, 'epoch': 0.58}
{'loss': 0.2448, 'grad_norm': 22.01757049560547, 'learning_rate': 4.361892583120204e-05, 'epoch': 0.64}
{'loss': 0.2449, 'grad_norm': 18.254314422607422, 'learning_rate': 4.297953964194373e-05, 'epoch': 0.7}
{'loss': 0.2129, 'grad_norm': 17.113534927368164, 'learning_rate': 4.234015345268542e-05, 'epoch': 0.77}
{'loss': 0.2365, 'grad_norm': 11.356915473937988, 'learning_rate': 4.1700767263427106e-05, 'epoch': 0.83}
{'loss': 0.2425, 'grad_norm': 16.349042892456055, 'learning_rate': 4.1061381074168794e-05, 'epoch': 0.9}
{'loss': 0.1974, 'grad_norm': 40.52045440673828, 'learning_rate': 4.042199488491048e-05, 'epoch': 0.96}
{'loss': 0.2055, 'grad_norm': 12.429720878601074, 'learning_rate': 3.978260869565217e-05, 'epoch': 1.02}
{'loss': 0.1327, 'grad_norm': 6.198179244995117, 'learning_rate': 3.914322250639386e-05, 'epoch': 1.09}
{'loss': 0.1646, 'grad_norm': 20.579744338989258, 'learning_rate': 3.8503836317135546e-05, 'epoch': 1.15}
{'loss': 0.1829, 'grad_norm': 9.410454750061035, 'learning_rate': 3.7864450127877235e-05, 'epoch': 1.21}
{'loss': 0.152, 'grad_norm': 9.695091247558594, 'learning_rate': 3.722506393861892e-05, 'epoch': 1.28}
{'loss': 0.1282, 'grad_norm': 5.278471946716309, 'learning_rate': 3.658567774936061e-05, 'epoch': 1.34}
{'loss': 0.157, 'grad_norm': 14.269668579101562, 'learning_rate': 3.5946291560102305e-05, 'epoch': 1.41}
{'loss': 0.1101, 'grad_norm': 19.31144905090332, 'learning_rate': 3.5306905370843993e-05, 'epoch': 1.47}
{'loss': 0.149, 'grad_norm': 0.8466778993606567, 'learning_rate': 3.466751918158568e-05, 'epoch': 1.53}
{'loss': 0.1312, 'grad_norm': 11.936384201049805, 'learning_rate': 3.402813299232737e-05, 'epoch': 1.6}
{'loss': 0.1785, 'grad_norm': 1.588842749595642, 'learning_rate': 3.338874680306906e-05, 'epoch': 1.66}
{'loss': 0.1293, 'grad_norm': 8.772869110107422, 'learning_rate': 3.2749360613810746e-05, 'epoch': 1.73}
{'loss': 0.1259, 'grad_norm': 17.97266387939453, 'learning_rate': 3.2109974424552434e-05, 'epoch': 1.79}
{'loss': 0.1281, 'grad_norm': 9.312459945678711, 'learning_rate': 3.147058823529412e-05, 'epoch': 1.85}
{'loss': 0.1436, 'grad_norm': 20.37181282043457, 'learning_rate': 3.083120204603581e-05, 'epoch': 1.92}
{'loss': 0.1467, 'grad_norm': 12.851072311401367, 'learning_rate': 3.0191815856777494e-05, 'epoch': 1.98}
{'loss': 0.0917, 'grad_norm': 3.4447438716888428, 'learning_rate': 2.9552429667519182e-05, 'epoch': 2.05}
{'loss': 0.0709, 'grad_norm': 3.1147162914276123, 'learning_rate': 2.891304347826087e-05, 'epoch': 2.11}
{'loss': 0.0788, 'grad_norm': 23.446826934814453, 'learning_rate': 2.827365728900256e-05, 'epoch': 2.17}
{'loss': 0.0781, 'grad_norm': 8.517889022827148, 'learning_rate': 2.7634271099744246e-05, 'epoch': 2.24}
{'loss': 0.0618, 'grad_norm': 26.142898559570312, 'learning_rate': 2.6994884910485935e-05, 'epoch': 2.3}
{'loss': 0.0721, 'grad_norm': 20.543975830078125, 'learning_rate': 2.6355498721227623e-05, 'epoch': 2.37}
{'loss': 0.0471, 'grad_norm': 1.266559362411499, 'learning_rate': 2.571611253196931e-05, 'epoch': 2.43}
{'loss': 0.0598, 'grad_norm': 25.13365936279297, 'learning_rate': 2.5076726342711e-05, 'epoch': 2.49}
{'loss': 0.0867, 'grad_norm': 14.791121482849121, 'learning_rate': 2.4437340153452687e-05, 'epoch': 2.56}
{'loss': 0.0707, 'grad_norm': 0.34120792150497437, 'learning_rate': 2.3797953964194375e-05, 'epoch': 2.62}
{'loss': 0.0486, 'grad_norm': 15.437049865722656, 'learning_rate': 2.3158567774936063e-05, 'epoch': 2.69}
{'loss': 0.0521, 'grad_norm': 0.5139016509056091, 'learning_rate': 2.251918158567775e-05, 'epoch': 2.75}
{'loss': 0.0707, 'grad_norm': 8.929424285888672, 'learning_rate': 2.187979539641944e-05, 'epoch': 2.81}
{'loss': 0.0577, 'grad_norm': 14.153059959411621, 'learning_rate': 2.1240409207161127e-05, 'epoch': 2.88}
{'loss': 0.0679, 'grad_norm': 14.581243515014648, 'learning_rate': 2.0601023017902815e-05, 'epoch': 2.94}
{'loss': 0.0565, 'grad_norm': 36.819217681884766, 'learning_rate': 1.9961636828644503e-05, 'epoch': 3.01}
{'loss': 0.0297, 'grad_norm': 45.92710876464844, 'learning_rate': 1.932225063938619e-05, 'epoch': 3.07}
{'loss': 0.037, 'grad_norm': 0.05145224556326866, 'learning_rate': 1.868286445012788e-05, 'epoch': 3.13}
{'loss': 0.033, 'grad_norm': 6.064523220062256, 'learning_rate': 1.8043478260869567e-05, 'epoch': 3.2}
{'loss': 0.0238, 'grad_norm': 4.626680850982666, 'learning_rate': 1.740409207161125e-05, 'epoch': 3.26}
{'loss': 0.0161, 'grad_norm': 0.0004689721390604973, 'learning_rate': 1.676470588235294e-05, 'epoch': 3.32}
{'loss': 0.0225, 'grad_norm': 0.9413995146751404, 'learning_rate': 1.6125319693094628e-05, 'epoch': 3.39}
{'loss': 0.022, 'grad_norm': 0.08299625664949417, 'learning_rate': 1.5485933503836316e-05, 'epoch': 3.45}
{'loss': 0.0281, 'grad_norm': 0.008915632963180542, 'learning_rate': 1.4846547314578006e-05, 'epoch': 3.52}
{'loss': 0.0377, 'grad_norm': 0.03419538214802742, 'learning_rate': 1.4207161125319694e-05, 'epoch': 3.58}
{'loss': 0.0381, 'grad_norm': 11.800586700439453, 'learning_rate': 1.3567774936061382e-05, 'epoch': 3.64}
{'loss': 0.0217, 'grad_norm': 0.009968705475330353, 'learning_rate': 1.292838874680307e-05, 'epoch': 3.71}
{'loss': 0.0175, 'grad_norm': 11.301794052124023, 'learning_rate': 1.2289002557544758e-05, 'epoch': 3.77}
{'loss': 0.0173, 'grad_norm': 0.47646206617355347, 'learning_rate': 1.1649616368286446e-05, 'epoch': 3.84}
{'loss': 0.006, 'grad_norm': 0.47416263818740845, 'learning_rate': 1.1010230179028134e-05, 'epoch': 3.9}
{'loss': 0.024, 'grad_norm': 5.3087438573129475e-05, 'learning_rate': 1.0370843989769822e-05, 'epoch': 3.96}
{'loss': 0.0148, 'grad_norm': 0.8005814552307129, 'learning_rate': 9.73145780051151e-06, 'epoch': 4.03}
{'loss': 0.0062, 'grad_norm': 0.011361543089151382, 'learning_rate': 9.092071611253198e-06, 'epoch': 4.09}
{'loss': 0.0006, 'grad_norm': 0.7299550771713257, 'learning_rate': 8.452685421994886e-06, 'epoch': 4.16}
{'loss': 0.0012, 'grad_norm': 0.0374058373272419, 'learning_rate': 7.813299232736574e-06, 'epoch': 4.22}
{'loss': 0.0029, 'grad_norm': 0.03270852193236351, 'learning_rate': 7.173913043478261e-06, 'epoch': 4.28}
{'loss': 0.0055, 'grad_norm': 0.5543050765991211, 'learning_rate': 6.534526854219949e-06, 'epoch': 4.35}
{'loss': 0.0036, 'grad_norm': 3.132487472612411e-05, 'learning_rate': 5.895140664961637e-06, 'epoch': 4.41}
{'loss': 0.0073, 'grad_norm': 0.0006208235281519592, 'learning_rate': 5.255754475703325e-06, 'epoch': 4.48}
{'loss': 0.0119, 'grad_norm': 0.013143867254257202, 'learning_rate': 4.616368286445013e-06, 'epoch': 4.54}
{'loss': 0.0001, 'grad_norm': 4.08373489335645e-06, 'learning_rate': 3.976982097186701e-06, 'epoch': 4.6}
{'loss': 0.0003, 'grad_norm': 2.4948176360339858e-06, 'learning_rate': 3.337595907928389e-06, 'epoch': 4.67}
{'loss': 0.0008, 'grad_norm': 7.595682621002197, 'learning_rate': 2.698209718670077e-06, 'epoch': 4.73}
{'loss': 0.001, 'grad_norm': 3.825752230568469e-07, 'learning_rate': 2.058823529411765e-06, 'epoch': 4.8}
{'loss': 0.0011, 'grad_norm': 0.000214431929634884, 'learning_rate': 1.4194373401534528e-06, 'epoch': 4.86}
{'loss': 0.0006, 'grad_norm': 2.5319750420749187e-05, 'learning_rate': 7.800511508951407e-07, 'epoch': 4.92}
{'loss': 0.0001, 'grad_norm': 0.045259393751621246, 'learning_rate': 1.4066496163682866e-07, 'epoch': 4.99}
{'train_runtime': 635.8797, 'train_samples_per_second': 196.578, 'train_steps_per_second': 6.149, 'train_loss': 0.12134349725982624, 'epoch': 5.0}
Evaluating on Test A (After Training B)...
Accuracy/Loss on Test A (After Training B): {'eval_loss': 14.990715980529785, 'eval_model_preparation_time': 0.0074, 'eval_runtime': 16.0095, 'eval_samples_per_second': 312.314, 'eval_steps_per_second': 39.039}

==================== EXPERIMENT 2: PEFT / LoRA ====================
trainable params: 589,824 || all params: 88,053,514 || trainable%: 0.6698
Training LoRA on Task A...
{'loss': 2.199, 'grad_norm': 1.42457914352417, 'learning_rate': 4.937340153452686e-05, 'epoch': 0.06}
{'loss': 1.568, 'grad_norm': 2.1200225353240967, 'learning_rate': 4.8734015345268545e-05, 'epoch': 0.13}
{'loss': 0.9674, 'grad_norm': 4.983283996582031, 'learning_rate': 4.809462915601023e-05, 'epoch': 0.19}
{'loss': 0.6291, 'grad_norm': 2.9287915229797363, 'learning_rate': 4.745524296675192e-05, 'epoch': 0.26}
{'loss': 0.4857, 'grad_norm': 5.054485321044922, 'learning_rate': 4.681585677749361e-05, 'epoch': 0.32}
{'loss': 0.3659, 'grad_norm': 10.964334487915039, 'learning_rate': 4.61764705882353e-05, 'epoch': 0.38}
{'loss': 0.3429, 'grad_norm': 8.218583106994629, 'learning_rate': 4.5537084398976985e-05, 'epoch': 0.45}
{'loss': 0.2606, 'grad_norm': 4.77737283706665, 'learning_rate': 4.489769820971867e-05, 'epoch': 0.51}
{'loss': 0.2221, 'grad_norm': 1.9189071655273438, 'learning_rate': 4.4258312020460354e-05, 'epoch': 0.58}
{'loss': 0.1898, 'grad_norm': 2.0877552032470703, 'learning_rate': 4.361892583120204e-05, 'epoch': 0.64}
{'loss': 0.1546, 'grad_norm': 0.7863981127738953, 'learning_rate': 4.297953964194373e-05, 'epoch': 0.7}
{'loss': 0.1514, 'grad_norm': 7.978950500488281, 'learning_rate': 4.234015345268542e-05, 'epoch': 0.77}
{'loss': 0.1191, 'grad_norm': 9.677409172058105, 'learning_rate': 4.1700767263427106e-05, 'epoch': 0.83}
{'loss': 0.1111, 'grad_norm': 8.9166259765625, 'learning_rate': 4.1061381074168794e-05, 'epoch': 0.9}
{'loss': 0.1049, 'grad_norm': 1.3468925952911377, 'learning_rate': 4.042199488491048e-05, 'epoch': 0.96}
{'loss': 0.1096, 'grad_norm': 0.21902620792388916, 'learning_rate': 3.978260869565217e-05, 'epoch': 1.02}
{'loss': 0.0888, 'grad_norm': 6.609984397888184, 'learning_rate': 3.914322250639386e-05, 'epoch': 1.09}
{'loss': 0.1093, 'grad_norm': 4.166210174560547, 'learning_rate': 3.8503836317135546e-05, 'epoch': 1.15}
{'loss': 0.0788, 'grad_norm': 0.24440456926822662, 'learning_rate': 3.7864450127877235e-05, 'epoch': 1.21}
{'loss': 0.0755, 'grad_norm': 3.2664613723754883, 'learning_rate': 3.722506393861892e-05, 'epoch': 1.28}
{'loss': 0.0725, 'grad_norm': 1.036368489265442, 'learning_rate': 3.658567774936061e-05, 'epoch': 1.34}
{'loss': 0.071, 'grad_norm': 2.7425365447998047, 'learning_rate': 3.5946291560102305e-05, 'epoch': 1.41}
{'loss': 0.0955, 'grad_norm': 10.380568504333496, 'learning_rate': 3.5306905370843993e-05, 'epoch': 1.47}
{'loss': 0.0902, 'grad_norm': 7.005435943603516, 'learning_rate': 3.466751918158568e-05, 'epoch': 1.53}
{'loss': 0.0713, 'grad_norm': 0.35471856594085693, 'learning_rate': 3.402813299232737e-05, 'epoch': 1.6}
{'loss': 0.0709, 'grad_norm': 1.116937518119812, 'learning_rate': 3.338874680306906e-05, 'epoch': 1.66}
{'loss': 0.0739, 'grad_norm': 8.544910430908203, 'learning_rate': 3.2749360613810746e-05, 'epoch': 1.73}
{'loss': 0.0428, 'grad_norm': 11.301989555358887, 'learning_rate': 3.2109974424552434e-05, 'epoch': 1.79}
{'loss': 0.079, 'grad_norm': 3.908921241760254, 'learning_rate': 3.147058823529412e-05, 'epoch': 1.85}
{'loss': 0.072, 'grad_norm': 4.162964820861816, 'learning_rate': 3.083120204603581e-05, 'epoch': 1.92}
{'loss': 0.0781, 'grad_norm': 8.919048309326172, 'learning_rate': 3.0191815856777494e-05, 'epoch': 1.98}
{'loss': 0.0564, 'grad_norm': 1.1392064094543457, 'learning_rate': 2.9552429667519182e-05, 'epoch': 2.05}
{'loss': 0.0318, 'grad_norm': 0.15735851228237152, 'learning_rate': 2.891304347826087e-05, 'epoch': 2.11}
{'loss': 0.0393, 'grad_norm': 0.08178982883691788, 'learning_rate': 2.827365728900256e-05, 'epoch': 2.17}
{'loss': 0.0516, 'grad_norm': 1.2604601383209229, 'learning_rate': 2.7634271099744246e-05, 'epoch': 2.24}
{'loss': 0.033, 'grad_norm': 1.360767126083374, 'learning_rate': 2.6994884910485935e-05, 'epoch': 2.3}
{'loss': 0.0487, 'grad_norm': 2.272603988647461, 'learning_rate': 2.6355498721227623e-05, 'epoch': 2.37}
{'loss': 0.0674, 'grad_norm': 8.205262184143066, 'learning_rate': 2.571611253196931e-05, 'epoch': 2.43}
{'loss': 0.0428, 'grad_norm': 9.980729103088379, 'learning_rate': 2.5076726342711e-05, 'epoch': 2.49}
{'loss': 0.0615, 'grad_norm': 0.13019627332687378, 'learning_rate': 2.4437340153452687e-05, 'epoch': 2.56}
{'loss': 0.0586, 'grad_norm': 1.5987874269485474, 'learning_rate': 2.3797953964194375e-05, 'epoch': 2.62}
{'loss': 0.0566, 'grad_norm': 2.6550955772399902, 'learning_rate': 2.3158567774936063e-05, 'epoch': 2.69}
{'loss': 0.0562, 'grad_norm': 10.148290634155273, 'learning_rate': 2.251918158567775e-05, 'epoch': 2.75}
{'loss': 0.0241, 'grad_norm': 0.09296879172325134, 'learning_rate': 2.187979539641944e-05, 'epoch': 2.81}
{'loss': 0.0728, 'grad_norm': 0.25556617975234985, 'learning_rate': 2.1240409207161127e-05, 'epoch': 2.88}
{'loss': 0.0471, 'grad_norm': 0.20618301630020142, 'learning_rate': 2.0601023017902815e-05, 'epoch': 2.94}
{'loss': 0.0589, 'grad_norm': 0.07395726442337036, 'learning_rate': 1.9961636828644503e-05, 'epoch': 3.01}
{'loss': 0.0284, 'grad_norm': 0.12392598390579224, 'learning_rate': 1.932225063938619e-05, 'epoch': 3.07}
{'loss': 0.0438, 'grad_norm': 0.0406372994184494, 'learning_rate': 1.868286445012788e-05, 'epoch': 3.13}
{'loss': 0.0337, 'grad_norm': 0.12169165909290314, 'learning_rate': 1.8043478260869567e-05, 'epoch': 3.2}
{'loss': 0.0402, 'grad_norm': 0.9283896088600159, 'learning_rate': 1.740409207161125e-05, 'epoch': 3.26}
{'loss': 0.0363, 'grad_norm': 1.4836210012435913, 'learning_rate': 1.676470588235294e-05, 'epoch': 3.32}
{'loss': 0.0373, 'grad_norm': 1.1498504877090454, 'learning_rate': 1.6125319693094628e-05, 'epoch': 3.39}
{'loss': 0.034, 'grad_norm': 0.1948666125535965, 'learning_rate': 1.5485933503836316e-05, 'epoch': 3.45}
{'loss': 0.0274, 'grad_norm': 4.80038595199585, 'learning_rate': 1.4846547314578006e-05, 'epoch': 3.52}
{'loss': 0.0294, 'grad_norm': 5.214944362640381, 'learning_rate': 1.4207161125319694e-05, 'epoch': 3.58}
{'loss': 0.0346, 'grad_norm': 1.381392240524292, 'learning_rate': 1.3567774936061382e-05, 'epoch': 3.64}
{'loss': 0.0366, 'grad_norm': 0.10512316226959229, 'learning_rate': 1.292838874680307e-05, 'epoch': 3.71}
{'loss': 0.0274, 'grad_norm': 18.184309005737305, 'learning_rate': 1.2289002557544758e-05, 'epoch': 3.77}
{'loss': 0.0301, 'grad_norm': 0.21435800194740295, 'learning_rate': 1.1649616368286446e-05, 'epoch': 3.84}
{'loss': 0.035, 'grad_norm': 6.8549885749816895, 'learning_rate': 1.1010230179028134e-05, 'epoch': 3.9}
{'loss': 0.042, 'grad_norm': 0.015112957917153835, 'learning_rate': 1.0370843989769822e-05, 'epoch': 3.96}
{'loss': 0.0402, 'grad_norm': 19.3818416595459, 'learning_rate': 9.73145780051151e-06, 'epoch': 4.03}
{'loss': 0.0273, 'grad_norm': 3.7254445552825928, 'learning_rate': 9.092071611253198e-06, 'epoch': 4.09}
{'loss': 0.0219, 'grad_norm': 0.10893402993679047, 'learning_rate': 8.452685421994886e-06, 'epoch': 4.16}
{'loss': 0.0469, 'grad_norm': 0.20575833320617676, 'learning_rate': 7.813299232736574e-06, 'epoch': 4.22}
{'loss': 0.0291, 'grad_norm': 0.7109932899475098, 'learning_rate': 7.173913043478261e-06, 'epoch': 4.28}
{'loss': 0.0206, 'grad_norm': 5.558712005615234, 'learning_rate': 6.534526854219949e-06, 'epoch': 4.35}
{'loss': 0.0256, 'grad_norm': 5.310607433319092, 'learning_rate': 5.895140664961637e-06, 'epoch': 4.41}
{'loss': 0.0259, 'grad_norm': 5.069961071014404, 'learning_rate': 5.255754475703325e-06, 'epoch': 4.48}
{'loss': 0.0284, 'grad_norm': 6.711158275604248, 'learning_rate': 4.616368286445013e-06, 'epoch': 4.54}
{'loss': 0.0205, 'grad_norm': 0.017776070162653923, 'learning_rate': 3.976982097186701e-06, 'epoch': 4.6}
{'loss': 0.0362, 'grad_norm': 0.03395232558250427, 'learning_rate': 3.337595907928389e-06, 'epoch': 4.67}
{'loss': 0.0163, 'grad_norm': 1.6115134954452515, 'learning_rate': 2.698209718670077e-06, 'epoch': 4.73}
{'loss': 0.0201, 'grad_norm': 11.779511451721191, 'learning_rate': 2.058823529411765e-06, 'epoch': 4.8}
{'loss': 0.0198, 'grad_norm': 0.2677482068538666, 'learning_rate': 1.4194373401534528e-06, 'epoch': 4.86}
{'loss': 0.0267, 'grad_norm': 9.974519729614258, 'learning_rate': 7.800511508951407e-07, 'epoch': 4.92}
{'loss': 0.0254, 'grad_norm': 0.2123868763446808, 'learning_rate': 1.4066496163682866e-07, 'epoch': 4.99}
{'train_runtime': 567.4068, 'train_samples_per_second': 220.301, 'train_steps_per_second': 6.891, 'train_loss': 0.13950722306357016, 'epoch': 5.0}
Training LoRA on Task B...
{'loss': 3.8729, 'grad_norm': 1.9889112710952759, 'learning_rate': 4.937340153452686e-05, 'epoch': 0.06}
{'loss': 1.5428, 'grad_norm': 3.4162585735321045, 'learning_rate': 4.8734015345268545e-05, 'epoch': 0.13}
{'loss': 0.6099, 'grad_norm': 2.0618584156036377, 'learning_rate': 4.809462915601023e-05, 'epoch': 0.19}
{'loss': 0.2937, 'grad_norm': 2.2703163623809814, 'learning_rate': 4.745524296675192e-05, 'epoch': 0.26}
{'loss': 0.1732, 'grad_norm': 5.706872463226318, 'learning_rate': 4.681585677749361e-05, 'epoch': 0.32}
{'loss': 0.124, 'grad_norm': 7.446727275848389, 'learning_rate': 4.61764705882353e-05, 'epoch': 0.38}
{'loss': 0.1105, 'grad_norm': 9.291707038879395, 'learning_rate': 4.5537084398976985e-05, 'epoch': 0.45}
{'loss': 0.0933, 'grad_norm': 3.0361669063568115, 'learning_rate': 4.489769820971867e-05, 'epoch': 0.51}
{'loss': 0.0852, 'grad_norm': 6.247434139251709, 'learning_rate': 4.4258312020460354e-05, 'epoch': 0.58}
{'loss': 0.0675, 'grad_norm': 1.4407694339752197, 'learning_rate': 4.361892583120204e-05, 'epoch': 0.64}
{'loss': 0.0432, 'grad_norm': 8.91403579711914, 'learning_rate': 4.297953964194373e-05, 'epoch': 0.7}
{'loss': 0.0616, 'grad_norm': 0.24323095381259918, 'learning_rate': 4.234015345268542e-05, 'epoch': 0.77}
{'loss': 0.0601, 'grad_norm': 6.060014247894287, 'learning_rate': 4.1700767263427106e-05, 'epoch': 0.83}
{'loss': 0.0904, 'grad_norm': 6.490545749664307, 'learning_rate': 4.1061381074168794e-05, 'epoch': 0.9}
{'loss': 0.0453, 'grad_norm': 4.488615989685059, 'learning_rate': 4.042199488491048e-05, 'epoch': 0.96}
{'loss': 0.0805, 'grad_norm': 1.19037926197052, 'learning_rate': 3.978260869565217e-05, 'epoch': 1.02}
{'loss': 0.0373, 'grad_norm': 0.0508866012096405, 'learning_rate': 3.914322250639386e-05, 'epoch': 1.09}
{'loss': 0.0407, 'grad_norm': 6.250194072723389, 'learning_rate': 3.8503836317135546e-05, 'epoch': 1.15}
{'loss': 0.0587, 'grad_norm': 0.4062563180923462, 'learning_rate': 3.7864450127877235e-05, 'epoch': 1.21}
{'loss': 0.0424, 'grad_norm': 0.6493868827819824, 'learning_rate': 3.722506393861892e-05, 'epoch': 1.28}
{'loss': 0.0326, 'grad_norm': 3.906912088394165, 'learning_rate': 3.658567774936061e-05, 'epoch': 1.34}
{'loss': 0.0402, 'grad_norm': 4.518426418304443, 'learning_rate': 3.5946291560102305e-05, 'epoch': 1.41}
{'loss': 0.0253, 'grad_norm': 1.6468150615692139, 'learning_rate': 3.5306905370843993e-05, 'epoch': 1.47}
{'loss': 0.0405, 'grad_norm': 0.07741823047399521, 'learning_rate': 3.466751918158568e-05, 'epoch': 1.53}
{'loss': 0.0264, 'grad_norm': 0.5151082873344421, 'learning_rate': 3.402813299232737e-05, 'epoch': 1.6}
{'loss': 0.0198, 'grad_norm': 0.02777276746928692, 'learning_rate': 3.338874680306906e-05, 'epoch': 1.66}
{'loss': 0.0484, 'grad_norm': 0.028963394463062286, 'learning_rate': 3.2749360613810746e-05, 'epoch': 1.73}
{'loss': 0.0472, 'grad_norm': 7.270580291748047, 'learning_rate': 3.2109974424552434e-05, 'epoch': 1.79}
{'loss': 0.0228, 'grad_norm': 0.44073110818862915, 'learning_rate': 3.147058823529412e-05, 'epoch': 1.85}
{'loss': 0.0349, 'grad_norm': 0.01800248771905899, 'learning_rate': 3.083120204603581e-05, 'epoch': 1.92}
{'loss': 0.0404, 'grad_norm': 5.011851787567139, 'learning_rate': 3.0191815856777494e-05, 'epoch': 1.98}
{'loss': 0.047, 'grad_norm': 0.043307721614837646, 'learning_rate': 2.9552429667519182e-05, 'epoch': 2.05}
{'loss': 0.0198, 'grad_norm': 0.11974048614501953, 'learning_rate': 2.891304347826087e-05, 'epoch': 2.11}
{'loss': 0.0112, 'grad_norm': 3.8997280597686768, 'learning_rate': 2.827365728900256e-05, 'epoch': 2.17}
{'loss': 0.0298, 'grad_norm': 0.06627669185400009, 'learning_rate': 2.7634271099744246e-05, 'epoch': 2.24}
{'loss': 0.0118, 'grad_norm': 1.056567668914795, 'learning_rate': 2.6994884910485935e-05, 'epoch': 2.3}
{'loss': 0.0272, 'grad_norm': 17.934654235839844, 'learning_rate': 2.6355498721227623e-05, 'epoch': 2.37}
{'loss': 0.0203, 'grad_norm': 0.006826970726251602, 'learning_rate': 2.571611253196931e-05, 'epoch': 2.43}
{'loss': 0.0127, 'grad_norm': 0.022524798288941383, 'learning_rate': 2.5076726342711e-05, 'epoch': 2.49}
{'loss': 0.0266, 'grad_norm': 1.0716218948364258, 'learning_rate': 2.4437340153452687e-05, 'epoch': 2.56}
{'loss': 0.0399, 'grad_norm': 0.18075162172317505, 'learning_rate': 2.3797953964194375e-05, 'epoch': 2.62}
{'loss': 0.0073, 'grad_norm': 0.033024776726961136, 'learning_rate': 2.3158567774936063e-05, 'epoch': 2.69}
{'loss': 0.0277, 'grad_norm': 0.00682286499068141, 'learning_rate': 2.251918158567775e-05, 'epoch': 2.75}
{'loss': 0.0154, 'grad_norm': 3.94159197807312, 'learning_rate': 2.187979539641944e-05, 'epoch': 2.81}
{'loss': 0.0208, 'grad_norm': 0.13127517700195312, 'learning_rate': 2.1240409207161127e-05, 'epoch': 2.88}
{'loss': 0.0299, 'grad_norm': 5.715610504150391, 'learning_rate': 2.0601023017902815e-05, 'epoch': 2.94}
{'loss': 0.0183, 'grad_norm': 0.37345725297927856, 'learning_rate': 1.9961636828644503e-05, 'epoch': 3.01}
{'loss': 0.0024, 'grad_norm': 0.016825880855321884, 'learning_rate': 1.932225063938619e-05, 'epoch': 3.07}
{'loss': 0.0174, 'grad_norm': 0.08837151527404785, 'learning_rate': 1.868286445012788e-05, 'epoch': 3.13}
{'loss': 0.0104, 'grad_norm': 0.0076771508902311325, 'learning_rate': 1.8043478260869567e-05, 'epoch': 3.2}
{'loss': 0.0186, 'grad_norm': 0.02550038881599903, 'learning_rate': 1.740409207161125e-05, 'epoch': 3.26}
{'loss': 0.0027, 'grad_norm': 0.018751434981822968, 'learning_rate': 1.676470588235294e-05, 'epoch': 3.32}
{'loss': 0.003, 'grad_norm': 0.006217006128281355, 'learning_rate': 1.6125319693094628e-05, 'epoch': 3.39}
{'loss': 0.0171, 'grad_norm': 0.024380439892411232, 'learning_rate': 1.5485933503836316e-05, 'epoch': 3.45}
{'loss': 0.0217, 'grad_norm': 0.02244960516691208, 'learning_rate': 1.4846547314578006e-05, 'epoch': 3.52}
{'loss': 0.0128, 'grad_norm': 0.03200586140155792, 'learning_rate': 1.4207161125319694e-05, 'epoch': 3.58}
{'loss': 0.0162, 'grad_norm': 0.006480975542217493, 'learning_rate': 1.3567774936061382e-05, 'epoch': 3.64}
{'loss': 0.0205, 'grad_norm': 0.003440020140260458, 'learning_rate': 1.292838874680307e-05, 'epoch': 3.71}
{'loss': 0.0248, 'grad_norm': 0.08800290524959564, 'learning_rate': 1.2289002557544758e-05, 'epoch': 3.77}
{'loss': 0.0221, 'grad_norm': 0.031828928738832474, 'learning_rate': 1.1649616368286446e-05, 'epoch': 3.84}
{'loss': 0.0096, 'grad_norm': 1.6731728315353394, 'learning_rate': 1.1010230179028134e-05, 'epoch': 3.9}
{'loss': 0.0136, 'grad_norm': 0.004339700099080801, 'learning_rate': 1.0370843989769822e-05, 'epoch': 3.96}
{'loss': 0.0088, 'grad_norm': 0.004651046358048916, 'learning_rate': 9.73145780051151e-06, 'epoch': 4.03}
{'loss': 0.0024, 'grad_norm': 0.05343606695532799, 'learning_rate': 9.092071611253198e-06, 'epoch': 4.09}
{'loss': 0.0026, 'grad_norm': 0.009335504844784737, 'learning_rate': 8.452685421994886e-06, 'epoch': 4.16}
{'loss': 0.0047, 'grad_norm': 0.03642123565077782, 'learning_rate': 7.813299232736574e-06, 'epoch': 4.22}
{'loss': 0.0179, 'grad_norm': 0.0070990691892802715, 'learning_rate': 7.173913043478261e-06, 'epoch': 4.28}
{'loss': 0.0127, 'grad_norm': 0.030519958585500717, 'learning_rate': 6.534526854219949e-06, 'epoch': 4.35}
{'loss': 0.0037, 'grad_norm': 0.011474426835775375, 'learning_rate': 5.895140664961637e-06, 'epoch': 4.41}
{'loss': 0.0075, 'grad_norm': 0.0830620527267456, 'learning_rate': 5.255754475703325e-06, 'epoch': 4.48}
{'loss': 0.0104, 'grad_norm': 0.29502254724502563, 'learning_rate': 4.616368286445013e-06, 'epoch': 4.54}
{'loss': 0.0124, 'grad_norm': 0.14105287194252014, 'learning_rate': 3.976982097186701e-06, 'epoch': 4.6}
{'loss': 0.0066, 'grad_norm': 0.3387549817562103, 'learning_rate': 3.337595907928389e-06, 'epoch': 4.67}
{'loss': 0.0096, 'grad_norm': 0.32335665822029114, 'learning_rate': 2.698209718670077e-06, 'epoch': 4.73}
{'loss': 0.005, 'grad_norm': 0.30926328897476196, 'learning_rate': 2.058823529411765e-06, 'epoch': 4.8}
{'loss': 0.0094, 'grad_norm': 0.02916218526661396, 'learning_rate': 1.4194373401534528e-06, 'epoch': 4.86}
{'loss': 0.013, 'grad_norm': 0.05318537726998329, 'learning_rate': 7.800511508951407e-07, 'epoch': 4.92}
{'loss': 0.0082, 'grad_norm': 0.003354360582306981, 'learning_rate': 1.4066496163682866e-07, 'epoch': 4.99}
{'train_runtime': 548.8333, 'train_samples_per_second': 227.756, 'train_steps_per_second': 7.124, 'train_loss': 0.11037193039989532, 'epoch': 5.0}
Evaluating on Test A (LoRA - After Training B)...
Accuracy/Loss on Test A (LoRA - After Training B): {'eval_loss': 8.368017196655273, 'eval_model_preparation_time': 0.0104, 'eval_runtime': 18.2735, 'eval_samples_per_second': 273.62, 'eval_steps_per_second': 34.202}

==================== FINAL RESULTS & PLOTTING ====================
Baseline - Accuracy on Task A after learning B: 14.9907
LoRA     - Accuracy on Task A after learning B: 8.3680
Generating comparison graph...
Graph saved as comparison_graph.png
